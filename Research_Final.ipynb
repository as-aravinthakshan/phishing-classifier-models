{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DFscmWH7vBt"
      },
      "source": [
        "## RESNET IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwl9Rh7DsEbE",
        "outputId": "2d674a41-fef1-46f3-d400-9ca02c9213b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Accuracy: 0.9528, Precision: 0.9498, Recall: 0.9659, F1: 0.9578\n",
            "Test - Accuracy: 0.9507, Precision: 0.9491, Recall: 0.9649, F1: 0.9569\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Drop the index column\n",
        "dataset = dataset.drop(columns=['index'])\n",
        "\n",
        "# Convert target labels -1 to 0\n",
        "dataset['Result'] = dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = dataset.drop(columns=['Result'])\n",
        "y = dataset['Result']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Gradient Boosting model\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, X, y):\n",
        "    predictions = model.predict(X)\n",
        "    accuracy = accuracy_score(y, predictions)\n",
        "    precision = precision_score(y, predictions)\n",
        "    recall = recall_score(y, predictions)\n",
        "    f1 = f1_score(y, predictions)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate on training data\n",
        "train_accuracy, train_precision, train_recall, train_f1 = evaluate(model, X_train, y_train)\n",
        "print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate(model, X_test, y_test)\n",
        "print(f'Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w22kzTLl72MN"
      },
      "source": [
        "## SVM( LINEAR  ) IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Mymme1smOR",
        "outputId": "cf308bbe-b74a-4339-d21b-3e7f1fb9d8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Accuracy: 0.9284, Precision: 0.9263, Recall: 0.9461, F1: 0.9361\n",
            "Test - Accuracy: 0.9285, Precision: 0.9262, Recall: 0.9498, F1: 0.9378\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Drop the index column\n",
        "dataset = dataset.drop(columns=['index'])\n",
        "\n",
        "# Convert target labels -1 to 0\n",
        "dataset['Result'] = dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = dataset.drop(columns=['Result'])\n",
        "y = dataset['Result']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the SVM model\n",
        "model = SVC(kernel='linear', C=1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(model, X, y):\n",
        "    predictions = model.predict(X)\n",
        "    accuracy = accuracy_score(y, predictions)\n",
        "    precision = precision_score(y, predictions)\n",
        "    recall = recall_score(y, predictions)\n",
        "    f1 = f1_score(y, predictions)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate on training data\n",
        "train_accuracy, train_precision, train_recall, train_f1 = evaluate(model, X_train, y_train)\n",
        "print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate(model, X_test, y_test)\n",
        "print(f'Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16z53om38FlW"
      },
      "source": [
        "## CNN IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXzXLa7DtnbD",
        "outputId": "1d7ff802-3e64-44ee-f5f5-70dea9d7036b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 010, Loss: 0.6136\n",
            "Train - Accuracy: 0.7353, Precision: 0.6795, Recall: 0.9890, F1: 0.8055\n",
            "Epoch: 020, Loss: 0.4629\n",
            "Train - Accuracy: 0.8414, Precision: 0.8318, Recall: 0.8947, F1: 0.8621\n",
            "Epoch: 030, Loss: 0.3162\n",
            "Train - Accuracy: 0.8779, Precision: 0.8862, Recall: 0.8945, F1: 0.8904\n",
            "Epoch: 040, Loss: 0.2525\n",
            "Train - Accuracy: 0.8956, Precision: 0.9049, Recall: 0.9070, F1: 0.9060\n",
            "Epoch: 050, Loss: 0.2166\n",
            "Train - Accuracy: 0.9101, Precision: 0.9134, Recall: 0.9255, F1: 0.9194\n",
            "Epoch: 060, Loss: 0.1950\n",
            "Train - Accuracy: 0.9227, Precision: 0.9216, Recall: 0.9404, F1: 0.9309\n",
            "Epoch: 070, Loss: 0.1819\n",
            "Train - Accuracy: 0.9289, Precision: 0.9264, Recall: 0.9470, F1: 0.9365\n",
            "Epoch: 080, Loss: 0.1720\n",
            "Train - Accuracy: 0.9326, Precision: 0.9318, Recall: 0.9478, F1: 0.9397\n",
            "Epoch: 090, Loss: 0.1640\n",
            "Train - Accuracy: 0.9348, Precision: 0.9333, Recall: 0.9502, F1: 0.9417\n",
            "Epoch: 100, Loss: 0.1572\n",
            "Train - Accuracy: 0.9366, Precision: 0.9351, Recall: 0.9517, F1: 0.9433\n",
            "Epoch: 110, Loss: 0.1509\n",
            "Train - Accuracy: 0.9405, Precision: 0.9374, Recall: 0.9565, F1: 0.9469\n",
            "Epoch: 120, Loss: 0.1451\n",
            "Train - Accuracy: 0.9430, Precision: 0.9393, Recall: 0.9592, F1: 0.9491\n",
            "Epoch: 130, Loss: 0.1397\n",
            "Train - Accuracy: 0.9449, Precision: 0.9414, Recall: 0.9604, F1: 0.9508\n",
            "Epoch: 140, Loss: 0.1344\n",
            "Train - Accuracy: 0.9467, Precision: 0.9432, Recall: 0.9619, F1: 0.9524\n",
            "Epoch: 150, Loss: 0.1297\n",
            "Train - Accuracy: 0.9454, Precision: 0.9369, Recall: 0.9665, F1: 0.9515\n",
            "Epoch: 160, Loss: 0.1253\n",
            "Train - Accuracy: 0.9478, Precision: 0.9433, Recall: 0.9637, F1: 0.9534\n",
            "Epoch: 170, Loss: 0.1212\n",
            "Train - Accuracy: 0.9492, Precision: 0.9484, Recall: 0.9606, F1: 0.9545\n",
            "Epoch: 180, Loss: 0.1177\n",
            "Train - Accuracy: 0.9522, Precision: 0.9520, Recall: 0.9623, F1: 0.9571\n",
            "Epoch: 190, Loss: 0.1146\n",
            "Train - Accuracy: 0.9510, Precision: 0.9488, Recall: 0.9637, F1: 0.9562\n",
            "Epoch: 200, Loss: 0.1116\n",
            "Train - Accuracy: 0.9530, Precision: 0.9517, Recall: 0.9641, F1: 0.9578\n",
            "Test - Accuracy: 0.9426, Precision: 0.9386, Recall: 0.9618, F1: 0.9500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Drop the index column\n",
        "dataset = dataset.drop(columns=['index'])\n",
        "\n",
        "# Convert target labels -1 to 0\n",
        "dataset['Result'] = dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = dataset.drop(columns=['Result'])\n",
        "y = dataset['Result']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add a channel dimension\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)  # Add a channel dimension\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * (X_train.shape[2] // 4), 64)\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, define the loss function and the optimizer\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "def train_model():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(data, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(labels.cpu(), predicted.cpu())\n",
        "        precision = precision_score(labels.cpu(), predicted.cpu())\n",
        "        recall = recall_score(labels.cpu(), predicted.cpu())\n",
        "        f1 = f1_score(labels.cpu(), predicted.cpu())\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 201):\n",
        "    loss = train_model()\n",
        "    if epoch % 10 == 0:\n",
        "        train_accuracy, train_precision, train_recall, train_f1 = evaluate_model(X_train, y_train)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "        print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(X_test, y_test)\n",
        "print(f'Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqGLQmtH8MYx"
      },
      "source": [
        "## CNN Leaky Relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR1RWfgnuZ5H",
        "outputId": "4b6be60d-75b1-4e49-cc72-52816e3c8f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 010, Loss: 0.6390\n",
            "Train - Accuracy: 0.6203, Precision: 0.5936, Recall: 0.9986, F1: 0.7446\n",
            "Epoch: 020, Loss: 0.5150\n",
            "Train - Accuracy: 0.8217, Precision: 0.7955, Recall: 0.9131, F1: 0.8502\n",
            "Epoch: 030, Loss: 0.3543\n",
            "Train - Accuracy: 0.8786, Precision: 0.8741, Recall: 0.9123, F1: 0.8928\n",
            "Epoch: 040, Loss: 0.2661\n",
            "Train - Accuracy: 0.8950, Precision: 0.8974, Recall: 0.9151, F1: 0.9062\n",
            "Epoch: 050, Loss: 0.2270\n",
            "Train - Accuracy: 0.9101, Precision: 0.9138, Recall: 0.9251, F1: 0.9194\n",
            "Epoch: 060, Loss: 0.2013\n",
            "Train - Accuracy: 0.9198, Precision: 0.9159, Recall: 0.9419, F1: 0.9287\n",
            "Epoch: 070, Loss: 0.1849\n",
            "Train - Accuracy: 0.9267, Precision: 0.9225, Recall: 0.9474, F1: 0.9348\n",
            "Epoch: 080, Loss: 0.1741\n",
            "Train - Accuracy: 0.9288, Precision: 0.9265, Recall: 0.9466, F1: 0.9364\n",
            "Epoch: 090, Loss: 0.1650\n",
            "Train - Accuracy: 0.9310, Precision: 0.9289, Recall: 0.9482, F1: 0.9384\n",
            "Epoch: 100, Loss: 0.1567\n",
            "Train - Accuracy: 0.9336, Precision: 0.9319, Recall: 0.9496, F1: 0.9407\n",
            "Epoch: 110, Loss: 0.1494\n",
            "Train - Accuracy: 0.9366, Precision: 0.9340, Recall: 0.9529, F1: 0.9434\n",
            "Epoch: 120, Loss: 0.1430\n",
            "Train - Accuracy: 0.9387, Precision: 0.9360, Recall: 0.9547, F1: 0.9453\n",
            "Epoch: 130, Loss: 0.1373\n",
            "Train - Accuracy: 0.9411, Precision: 0.9392, Recall: 0.9555, F1: 0.9473\n",
            "Epoch: 140, Loss: 0.1321\n",
            "Train - Accuracy: 0.9445, Precision: 0.9465, Recall: 0.9537, F1: 0.9501\n",
            "Epoch: 150, Loss: 0.1276\n",
            "Train - Accuracy: 0.9464, Precision: 0.9469, Recall: 0.9570, F1: 0.9519\n",
            "Epoch: 160, Loss: 0.1234\n",
            "Train - Accuracy: 0.9475, Precision: 0.9456, Recall: 0.9606, F1: 0.9530\n",
            "Epoch: 170, Loss: 0.1197\n",
            "Train - Accuracy: 0.9491, Precision: 0.9482, Recall: 0.9606, F1: 0.9544\n",
            "Epoch: 180, Loss: 0.1162\n",
            "Train - Accuracy: 0.9518, Precision: 0.9480, Recall: 0.9661, F1: 0.9570\n",
            "Epoch: 190, Loss: 0.1129\n",
            "Train - Accuracy: 0.9536, Precision: 0.9543, Recall: 0.9625, F1: 0.9584\n",
            "Epoch: 200, Loss: 0.1100\n",
            "Train - Accuracy: 0.9543, Precision: 0.9525, Recall: 0.9657, F1: 0.9591\n",
            "Test - Accuracy: 0.9435, Precision: 0.9414, Recall: 0.9602, F1: 0.9507\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Drop the index column\n",
        "dataset = dataset.drop(columns=['index'])\n",
        "\n",
        "# Convert target labels -1 to 0\n",
        "dataset['Result'] = dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = dataset.drop(columns=['Result'])\n",
        "y = dataset['Result']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add a channel dimension\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)  # Add a channel dimension\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define the CNN model with LeakyReLU\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.fc1 = nn.Linear(32 * (X_train.shape[2] // 4), 64)\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(self.leaky_relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.leaky_relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, define the loss function and the optimizer\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "def train_model():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(data, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(labels.cpu(), predicted.cpu())\n",
        "        precision = precision_score(labels.cpu(), predicted.cpu())\n",
        "        recall = recall_score(labels.cpu(), predicted.cpu())\n",
        "        f1 = f1_score(labels.cpu(), predicted.cpu())\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 201):\n",
        "    loss = train_model()\n",
        "    if epoch % 10 == 0:\n",
        "        train_accuracy, train_precision, train_recall, train_f1 = evaluate_model(X_train, y_train)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "        print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(X_test, y_test)\n",
        "print(f'Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_G4a3v78QGR"
      },
      "source": [
        "## KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQC0hqNU3FZg",
        "outputId": "a3ca0b50-a849-492a-f229-0c5f4c7c96f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN Train - Accuracy: 0.9665, Precision: 0.9658, Recall: 0.9741, F1: 0.9699\n",
            "KNN Test - Accuracy: 0.9408, Precision: 0.9411, Recall: 0.9554, F1: 0.9482\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Drop the index column\n",
        "dataset = dataset.drop(columns=['index'])\n",
        "\n",
        "# Convert target labels -1 to 0\n",
        "dataset['Result'] = dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = dataset.drop(columns=['Result'])\n",
        "y = dataset['Result']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Initialize and train the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train.numpy(), y_train.numpy())\n",
        "\n",
        "# Make predictions on the train set\n",
        "y_train_pred = knn.predict(X_train.numpy())\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = knn.predict(X_test.numpy())\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_accuracy = accuracy_score(y_train.numpy(), y_train_pred)\n",
        "train_precision = precision_score(y_train.numpy(), y_train_pred)\n",
        "train_recall = recall_score(y_train.numpy(), y_train_pred)\n",
        "train_f1 = f1_score(y_train.numpy(), y_train_pred)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = accuracy_score(y_test.numpy(), y_test_pred)\n",
        "test_precision = precision_score(y_test.numpy(), y_test_pred)\n",
        "test_recall = recall_score(y_test.numpy(), y_test_pred)\n",
        "test_f1 = f1_score(y_test.numpy(), y_test_pred)\n",
        "\n",
        "print(f'KNN Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "print(f'KNN Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5181qdv8To8"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yChGNkPT3qZ7",
        "outputId": "af3a2d86-a91b-4d2d-87ed-41013fece162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 010, Loss: 0.8541\n",
            "Train - Accuracy: 0.3405, Precision: 0.4075, Recall: 0.4186, F1: 0.4130\n",
            "Epoch: 020, Loss: 0.8024\n",
            "Train - Accuracy: 0.3823, Precision: 0.4450, Recall: 0.4629, F1: 0.4538\n",
            "Epoch: 030, Loss: 0.7564\n",
            "Train - Accuracy: 0.4602, Precision: 0.5121, Recall: 0.5510, F1: 0.5309\n",
            "Epoch: 040, Loss: 0.7149\n",
            "Train - Accuracy: 0.5286, Precision: 0.5669, Recall: 0.6336, F1: 0.5984\n",
            "Epoch: 050, Loss: 0.6777\n",
            "Train - Accuracy: 0.5932, Precision: 0.6171, Recall: 0.7007, F1: 0.6563\n",
            "Epoch: 060, Loss: 0.6445\n",
            "Train - Accuracy: 0.6451, Precision: 0.6581, Recall: 0.7487, F1: 0.7004\n",
            "Epoch: 070, Loss: 0.6149\n",
            "Train - Accuracy: 0.6825, Precision: 0.6876, Recall: 0.7829, F1: 0.7322\n",
            "Epoch: 080, Loss: 0.5885\n",
            "Train - Accuracy: 0.7159, Precision: 0.7125, Recall: 0.8170, F1: 0.7612\n",
            "Epoch: 090, Loss: 0.5647\n",
            "Train - Accuracy: 0.7417, Precision: 0.7367, Recall: 0.8311, F1: 0.7811\n",
            "Epoch: 100, Loss: 0.5433\n",
            "Train - Accuracy: 0.7655, Precision: 0.7583, Recall: 0.8468, F1: 0.8001\n",
            "Epoch: 110, Loss: 0.5240\n",
            "Train - Accuracy: 0.7827, Precision: 0.7744, Recall: 0.8578, F1: 0.8140\n",
            "Epoch: 120, Loss: 0.5064\n",
            "Train - Accuracy: 0.7932, Precision: 0.7848, Recall: 0.8637, F1: 0.8224\n",
            "Epoch: 130, Loss: 0.4903\n",
            "Train - Accuracy: 0.8078, Precision: 0.7996, Recall: 0.8717, F1: 0.8341\n",
            "Epoch: 140, Loss: 0.4756\n",
            "Train - Accuracy: 0.8198, Precision: 0.8129, Recall: 0.8766, F1: 0.8435\n",
            "Epoch: 150, Loss: 0.4622\n",
            "Train - Accuracy: 0.8295, Precision: 0.8244, Recall: 0.8798, F1: 0.8512\n",
            "Epoch: 160, Loss: 0.4498\n",
            "Train - Accuracy: 0.8365, Precision: 0.8336, Recall: 0.8809, F1: 0.8566\n",
            "Epoch: 170, Loss: 0.4383\n",
            "Train - Accuracy: 0.8412, Precision: 0.8390, Recall: 0.8831, F1: 0.8605\n",
            "Epoch: 180, Loss: 0.4277\n",
            "Train - Accuracy: 0.8460, Precision: 0.8452, Recall: 0.8841, F1: 0.8642\n",
            "Epoch: 190, Loss: 0.4179\n",
            "Train - Accuracy: 0.8515, Precision: 0.8515, Recall: 0.8868, F1: 0.8688\n",
            "Epoch: 200, Loss: 0.4088\n",
            "Train - Accuracy: 0.8543, Precision: 0.8551, Recall: 0.8874, F1: 0.8710\n",
            "Test - Accuracy: 0.8593, Precision: 0.8636, Recall: 0.8932, F1: 0.8782\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate the model, define the loss function and the optimizer\n",
        "model = LogisticRegression(X_train.shape[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "def train_model():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(data, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(labels.cpu(), predicted.cpu())\n",
        "        precision = precision_score(labels.cpu(), predicted.cpu())\n",
        "        recall = recall_score(labels.cpu(), predicted.cpu())\n",
        "        f1 = f1_score(labels.cpu(), predicted.cpu())\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 201):\n",
        "    loss = train_model()\n",
        "    if epoch % 10 == 0:\n",
        "        train_accuracy, train_precision, train_recall, train_f1 = evaluate_model(X_train, y_train)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "        print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(X_test, y_test)\n",
        "print(f'Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1h-yYm8YW8"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46sDbZ830EX",
        "outputId": "545eda74-f35a-47d3-e34d-edccef713d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Train - Accuracy: 0.6030, Precision: 0.9957, Recall: 0.2850, F1: 0.4431\n",
            "Naive Bayes Test - Accuracy: 0.5798, Precision: 0.9970, Recall: 0.2606, F1: 0.4131\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Drop the index column\n",
        "dataset = dataset.drop(columns=['index'])\n",
        "\n",
        "# Convert target labels -1 to 0\n",
        "dataset['Result'] = dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = dataset.drop(columns=['Result'])\n",
        "y = dataset['Result']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Initialize and train the Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train.numpy(), y_train.numpy())\n",
        "\n",
        "# Make predictions on the train set\n",
        "y_train_pred = nb.predict(X_train.numpy())\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = nb.predict(X_test.numpy())\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_accuracy = accuracy_score(y_train.numpy(), y_train_pred)\n",
        "train_precision = precision_score(y_train.numpy(), y_train_pred)\n",
        "train_recall = recall_score(y_train.numpy(), y_train_pred)\n",
        "train_f1 = f1_score(y_train.numpy(), y_train_pred)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = accuracy_score(y_test.numpy(), y_test_pred)\n",
        "test_precision = precision_score(y_test.numpy(), y_test_pred)\n",
        "test_recall = recall_score(y_test.numpy(), y_test_pred)\n",
        "test_f1 = f1_score(y_test.numpy(), y_test_pred)\n",
        "\n",
        "print(f'Naive Bayes Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "print(f'Naive Bayes Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L-6LUx58doA"
      },
      "source": [
        "## NAIVE BAYES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_iJTHnMpbML",
        "outputId": "750194ce-3234-43ed-b6c9-b48adf546432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 010, Loss: 0.1572\n",
            "Train - Accuracy: 0.9268, Precision: 0.9122, Recall: 0.9604, F1: 0.9357\n",
            "Epoch: 020, Loss: 0.1197\n",
            "Train - Accuracy: 0.9439, Precision: 0.9311, Recall: 0.9706, F1: 0.9505\n",
            "Epoch: 030, Loss: 0.0793\n",
            "Train - Accuracy: 0.9638, Precision: 0.9528, Recall: 0.9835, F1: 0.9679\n",
            "Epoch: 040, Loss: 0.0474\n",
            "Train - Accuracy: 0.9729, Precision: 0.9594, Recall: 0.9931, F1: 0.9759\n",
            "Epoch: 050, Loss: 0.0304\n",
            "Train - Accuracy: 0.9773, Precision: 0.9633, Recall: 0.9969, F1: 0.9798\n",
            "Epoch: 060, Loss: 0.0228\n",
            "Train - Accuracy: 0.9885, Precision: 0.9831, Recall: 0.9963, F1: 0.9897\n",
            "Epoch: 070, Loss: 0.0208\n",
            "Train - Accuracy: 0.9895, Precision: 0.9906, Recall: 0.9904, F1: 0.9905\n",
            "Epoch: 080, Loss: 0.0195\n",
            "Train - Accuracy: 0.9902, Precision: 0.9869, Recall: 0.9955, F1: 0.9912\n",
            "Epoch: 090, Loss: 0.0189\n",
            "Train - Accuracy: 0.9905, Precision: 0.9884, Recall: 0.9945, F1: 0.9915\n",
            "Epoch: 100, Loss: 0.0181\n",
            "Train - Accuracy: 0.9902, Precision: 0.9892, Recall: 0.9931, F1: 0.9911\n",
            "Epoch: 110, Loss: 0.0230\n",
            "Train - Accuracy: 0.9903, Precision: 0.9900, Recall: 0.9925, F1: 0.9912\n",
            "Epoch: 120, Loss: 0.0195\n",
            "Train - Accuracy: 0.9905, Precision: 0.9896, Recall: 0.9933, F1: 0.9914\n",
            "Epoch: 130, Loss: 0.0182\n",
            "Train - Accuracy: 0.9906, Precision: 0.9890, Recall: 0.9941, F1: 0.9916\n",
            "Epoch: 140, Loss: 0.0178\n",
            "Train - Accuracy: 0.9906, Precision: 0.9896, Recall: 0.9935, F1: 0.9916\n",
            "Epoch: 150, Loss: 0.0177\n",
            "Train - Accuracy: 0.9905, Precision: 0.9884, Recall: 0.9945, F1: 0.9915\n",
            "Epoch: 160, Loss: 0.0176\n",
            "Train - Accuracy: 0.9906, Precision: 0.9896, Recall: 0.9935, F1: 0.9916\n",
            "Epoch: 170, Loss: 0.0210\n",
            "Train - Accuracy: 0.9895, Precision: 0.9896, Recall: 0.9914, F1: 0.9905\n",
            "Epoch: 180, Loss: 0.0192\n",
            "Train - Accuracy: 0.9898, Precision: 0.9924, Recall: 0.9892, F1: 0.9908\n",
            "Epoch: 190, Loss: 0.0181\n",
            "Train - Accuracy: 0.9904, Precision: 0.9882, Recall: 0.9945, F1: 0.9914\n",
            "Epoch: 200, Loss: 0.0178\n",
            "Train - Accuracy: 0.9905, Precision: 0.9878, Recall: 0.9951, F1: 0.9915\n",
            "Test - Accuracy: 0.9629, Precision: 0.9586, Recall: 0.9769, F1: 0.9676\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Drop the index column\n",
        "dataset = dataset.drop(columns=['index'])\n",
        "\n",
        "# Convert target labels -1 to 0\n",
        "dataset['Result'] = dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = dataset.drop(columns=['Result'])\n",
        "y = dataset['Result']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define the ResNet block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(in_features, in_features),\n",
        "            nn.BatchNorm1d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features, in_features),\n",
        "            nn.BatchNorm1d(in_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.layer(x)\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet model\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.input_layer = nn.Linear(num_features, 64)\n",
        "        self.block1 = ResidualBlock(64)\n",
        "        self.block2 = ResidualBlock(64)\n",
        "        self.block3 = ResidualBlock(64)\n",
        "        self.output_layer = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input_layer(x))\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.output_layer(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Model parameters\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = 2  # Phishing or not\n",
        "\n",
        "# Instantiate the model\n",
        "model = ResNet(num_features, num_classes)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training the model\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = criterion(output, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(data, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1)\n",
        "        accuracy = accuracy_score(labels.cpu(), pred.cpu())\n",
        "        precision = precision_score(labels.cpu(), pred.cpu())\n",
        "        recall = recall_score(labels.cpu(), pred.cpu())\n",
        "        f1 = f1_score(labels.cpu(), pred.cpu())\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 201):\n",
        "    loss = train()\n",
        "    if epoch % 10 == 0:\n",
        "        train_accuracy, train_precision, train_recall, train_f1 = evaluate(X_train, y_train)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "        print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy, test_precision, test_recall, test_f1 = evaluate(X_test, y_test)\n",
        "print(f'Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
